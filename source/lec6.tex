\documentclass[twoside,12pt]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}
\newcommand{\inprod}[2]{\ensuremath{\langle #1 , \, #2 \rangle}}


\usepackage{amsmath,amsfonts,graphicx}


\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf 36-705: Intermediate Statistics		\hfill Fall 2017} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

\newcommand{\cdist}{\overset{d}{\rightarrow}}
\newcommand{\edist}{\overset{d}{=}}

\newcommand{\cprob}{\overset{p}{\rightarrow}}


% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\E{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{6}{September 11}{Siva Balakrishnan}{Siva Balakrishnan}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

% **** YOUR NOTES GO HERE:
In today's lecture we will discuss some important extensions of the classical CLT.
\section{Only independence but not identically distributed}
Since this came up in lecture last time. 
The CLT goes through almost exactly as stated, however, we need conditions to ensure that one or a small number of random variables do not dominate the sum. There are many such results but the most classical is called the Lyapunov CLT. I will state something that is slightly weaker than the actual result. Lyapunov is one of the fathers of the theory of dynamical systems and a student of Chebyshev's. As is the case with Chebyshev, there are several foundational concepts that are named for him (the CLT is only one).

Define the variance of the average:
\begin{align*}
s_n^2 = \sum_{i=1}^n \sigma_i^2.
\end{align*}

{\bf Lyapunov CLT: } Suppose $X_1,\ldots,X_n$ are independent but not necessarily identically distributed. Let $\mu_i = \mathbb{E}[X_i]$ and let $\sigma_i = \text{Var}(X_i)$. Then if we satisfy the Lyapunov condition:
\begin{align*}
\lim_{n \rightarrow \infty} \frac{1}{s_n^{3}} \sum_{i=1}^n \mathbb{E} |X_i - \mu|^3 = 0,
\end{align*}
then 
\begin{align*}
\frac{1}{s_n} \sum_{i=1}^n \left[X_i - \mu_i \right] \cdist N(0,1).
\end{align*}

First notice that ignoring the Lyapunov condition, in the i.i.d case we just have $\mu_i = \mu$ and $s_n = \sqrt{n} \sigma$ so we get back our usual CLT.

It is worthwhile trying to understand the Lyapunov condition, and when it might be violated. In particular, consider the extreme case, when all the random variables are deterministic, except $X_1$ which has mean $\mu_1$ and variance $\sigma_1^2 > 0$. Then $s_n^3 = \sigma_1^3$ and the third absolute moment $\mathbb{E}|X_1 - \mu|^3 > 0$ so that the Lyapunov condition fails. Roughly, what can happen in the non-identically distributed case is that only one random variable can dominate the sum in which case you are not really averaging many things so you do not have a CLT.

On the other hand in a more typical case, one might have that the third absolute moments are bounded by some constant $C > 0$ say and the variance of any particular random variable is not too small. In this case,
\begin{align*}
s_n^2 &= \sum_{i=1}^n \sigma_i^2 \geq n \sigma_{\min}^2, 
\end{align*}
and
\begin{align*}
\sum_{i=1}^n \mathbb{E} |X_i - \mu|^3 & \leq Cn. 
\end{align*}
In this case, we will have that the Lyapunov ratio $\leq \frac{C}{\sqrt{n} \sigma_{\min}^3} \rightarrow 0$ so that the condition is indeed satisfied. 



\section{Multivariate CLT}
The first important extension is the multivariate CLT.

{\bf Multivariate CLT: } If $X_1,\ldots,X_n$ are i.i.d with mean $\mu \in \mathbb{R}^d$, and covariance matrix $\Sigma \in \mathbb{R}^{d \times d}$ (with finite entries) then,
\begin{align*}
\sqrt{n} (\widehat{\mu} - \mu) \cdist N(0,\Sigma).
\end{align*}

{\bf Notes: } 
\begin{enumerate}
\item You might wonder what convergence in distribution means for random vectors. A random vector still has a CDF, typically we define this as:
\begin{align*}
F_X(x_1,\ldots,x_d) = \mathbb{P}(X_1 \leq x_1, \ldots X_d \leq x_d),
\end{align*}
so we can still define convergence in distribution via pointwise convergence of the CDF. In order to define points of continuity it turns out that the correct definition is that a point is a point of continuity of the CDF if the boundary of the rectangle whose upper right corner is $(x_1,\ldots,x_d)$ has probability $0$. 
\item Although $d$ can be larger than $1$, it is taken to be fixed as $n \rightarrow \infty$. Central limit theorems, when $d$ is allowed to grow, i.e. high-dimensional CLTs are rare and are an active topic of research.
\item The proof of this result follows directly from the proof of the univariate CLT and a powerful result in asymptotic statistics known as the Cramer-Wold device. The Cramer-Wold device roughly asserts that if $a^T X_n \cdist a^T X$ for all vectors $a \in \mathbb{R}^d$ then $X_n \cdist X.$  
\end{enumerate}

\section{CLT with estimated variance}
We saw that in our typical use case of the CLT (constructing confidence intervals) we needed to know the variance $\sigma$. In practice, we most often do not know this. However, we can estimate this quantity in the usual way,
\begin{align*}
\widehat{\sigma}^2_n = \frac{1}{n-1} \sum_{i=1}^n (X_i - \widehat{\mu})^2.
\end{align*}
It turns out that we can replace the standard deviation in the CLT by $\widehat{\sigma}$ and still have the same convergence in distribution, i.e.
\begin{align*}
\frac{\sqrt{n}(\widehat{\mu}- \mu)}{\widehat{\sigma}_n} \cdist N(0,1).
\end{align*}

The proof follows from a sequence of applications of Slutsky's theorem and the continuous mapping theorem.

{\bf Proof: } First observe that if we can show that $\frac{\sigma}{\widehat{\sigma}_n} \cdist 1,$ then an application of Slutsky's theorem and the CLT gives us the desired result.

Since square-root is a continuous map, by the continuous mapping theorem, it suffices to show that $\frac{\sigma^2}{\widehat{\sigma}_n^2} \cdist 1.$ We will instead show the stronger statement that,
\begin{align*}
\widehat{\sigma}_n^2 \cprob \sigma^2,
\end{align*}
which implies the desired statement via the continuous mapping theorem (see Larry's notes for more details). Note that,
\begin{align*}
\widehat{\sigma}_n^2 &= \frac{1}{n-1} \sum_{i=1}^n (X_i - \widehat{\mu})^2 \\
&\cprob \frac{1}{n}  \sum_{i=1}^n (X_i - \widehat{\mu})^2, % = \frac{1}{n} \sum_{i=1}^n X_i^2 - \left(\frac{1}{n} \sum_{i=1}^n X_i \right)
\end{align*}
using the fact that $\frac{n-1}{n} \rightarrow 1$. Now,
\begin{align*}
\frac{1}{n}  \sum_{i=1}^n (X_i - \widehat{\mu})^2 = \frac{1}{n} \sum_{i=1}^n X_i^2 - \left(\frac{1}{n} \sum_{i=1}^n X_i \right)^2 \cprob \mathbb{E}[X^2] - (\mathbb{E}[X])^2
\end{align*}
using the WLLN. This concludes the proof.


\section{Rate of convergence in CLT - Berry Esseen}
While the central limit theorem is an asymptotic result (i.e. a statement about $n \rightarrow \infty$) it turns out that under fairly general conditions we can say how close to a standard normal the average is, in distribution, for finite values $n$. Such results are known as Berry Esseen bounds. Roughly, they are proved by carefully tracking the remainder terms in our Taylor series proof but we will not do this here.

{\bf Berry-Esseen: } Suppose that $X_1,\ldots,X_n \sim P$. Let $\mu = \mathbb{E}[X_1]$, $\sigma^2 = \mathbb{E}[(X_1 - \mu)^2]$, and $\mu_3 = \mathbb{E}[|X_1 - \mu|^3].$ 
Let 
\begin{align*}
F_n(x) = \mathbb{P}\left( \frac{\sqrt{n} (\widehat{\mu} - \mu)}{\sigma} \leq x \right),
\end{align*}
denote the CDF of the normalized sample average. 
If $\mu_3 < \infty$ then,
\begin{align*}
\sup_{x} | F_n(x) - \Phi(x)| \leq \frac{9 \mu_3}{\sigma^3 \sqrt{n}}.
\end{align*}

This bound is roughly saying that if $\mu_3/\sigma^3$ is small then the convergence to normality in distribution happens quite fast.


\section{The Delta Method}
A natural question that arises frequently is the following: suppose we have a sequence of random variables $X_n$ that converges in distribution to a Gaussian distribution then can we characterize the limiting distribution of $g(X_n)$ where $g$ is a smooth function?

We could work this out by using the continuous mapping theorem (indeed, that is at the heart of the proof we are about to give).

{\bf Delta Method: } Suppose that,
\begin{align*}
\frac{\sqrt{n} (X_n - \mu) }{\sigma} \cdist N(0,1),
\end{align*}
and that $g$ is a continuously differentiable function such that $g'(\mu) \neq 0$. Then,
\begin{align*}
\frac{\sqrt{n} (g(X_n) - g(\mu)) }{\sigma} \cdist N(0, [g'(\mu)]^2).
\end{align*}

{\bf Proof: } The basic idea is simply to use Taylor's approximation. We know that,
\begin{align*}
g(X_n) \approx g(\mu) + g'(\mu) (X_n - \mu),
\end{align*}
so that,
\begin{align*}
\frac{\sqrt{n} (g(X_n) - g(\mu))}{\sigma} \approx g'(\mu) \frac{ \sqrt{n} (X_n - \mu)}{\sigma} \cdist N(0, [g'(\mu)]^2).
\end{align*}
To be rigorous however we need to take care of the remainder terms. Here is a more formal proof.

By a rigorous application of Taylor's theorem we obtain,
\begin{align*}
\frac{\sqrt{n} (g(X_n) - g(\mu))}{\sigma} = g'(\widetilde{\mu}) \frac{ \sqrt{n} (X_n - \mu)}{\sigma},
\end{align*}
where $\widetilde{\mu}$ is on the line joining $\mu$ to $\widehat{\mu}$. We know by the WLLN that 
$\widehat{\mu} \cprob \mu$ and so $\widetilde{\mu} \cprob \mu$. Since $g$ is continuously differentiable, we can use the continuous mapping theorem to conclude that,
\begin{align*}
g'(\widetilde{\mu}) \cprob g'(\mu).
\end{align*}
Now, we apply Slutsky's theorem to obtain that,
\begin{align*}
 g'(\widetilde{\mu}) \frac{ \sqrt{n} (X_n - \mu)}{\sigma} \cdist  g'(\mu) N(0,1) \edist N(0, [g'(\mu)]^2).
\end{align*}


{\bf An example: } Suppose we have $X_1,\ldots,X_n \sim P$ with $\mathbb{E}[X] = \mu$, $\text{Var}(X) = \sigma^2 < \infty$. Suppose we are interested in the distribution of $Y_n = \exp( \widehat{\mu}_n).$ Using that fact that $g'(\mu) = \exp(\mu)$, applying the Delta method we obtain,
\begin{align*}
\sqrt{n} \left(\frac{\exp( \widehat{\mu}_n) - \exp(\mu)}{\sigma}\right) \cdist N(0, \exp(2\mu)).
\end{align*}


{\bf Multivariate Delta Method: } There is a multivariate analogue of the Delta method (which is likely where the name comes from?).
Suppose we have random vectors $X_1,\ldots,X_n \in \mathbb{R}^d$, and $g: \mathbb{R}^d \mapsto \mathbb{R}$ is a continuously differentiable function, then 
\begin{align*}
\sqrt{n} (g(\widehat{\mu}_n) - g(\mu)) \cdist N(0, \Delta_\mu(g)^T \Sigma \Delta_\mu(g)),
\end{align*}
where 
\begin{align*}
\Delta_g(\mu) = \left( \begin{array}{c} \frac{\partial g(x)}{\partial x_1} \\ \vdots \\ \frac{\partial g(x)}{\partial x_d}
\end{array} \right)_{x = \mu},
\end{align*}
is the gradient of $g$ evaluated at $\mu$.

\end{document}





