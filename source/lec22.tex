\documentclass[twoside,12pt]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}
\newcommand{\inprod}[2]{\ensuremath{\langle #1 , \, #2 \rangle}}


\usepackage{amsmath,amsfonts,graphicx}
\newcommand{\V}{\mbox{${\sf Var}$}}


\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf 36-705: Intermediate Statistics		\hfill Fall 2017} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Lecturer: #3 \hfill} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{example}[theorem]{Example}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

\newcommand{\cdist}{\overset{d}{\rightarrow}}
\newcommand{\edist}{\overset{d}{=}}

\newcommand{\cprob}{\overset{p}{\rightarrow}}
\newcommand{\tobs}{T(Z_{\text{obs}})}
\newcommand{\zobs}{Z_{\text{obs}}}
\newcommand{\permtest}{\phi_{\text{perm}}}




% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\E{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{22}{October 23}{Siva Balakrishnan}{Siva Balakrishnan}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

\section{Permutation cut-offs}
Since the idea of using permutations was not completely clear to some of you I have included a proof of the fact that it controls the Type I error as desired.

We observe $X_1,\ldots,X_{n_1} \sim P$ and $Y_1,\ldots,Y_{n_2} \sim Q$, and want to test if:
\begin{align*}
H_0: &~~~P = Q \\
H_1: &~~~P \neq Q.
\end{align*}


Let us introduce some notation: we suppose we are given a test statistic $T$ which is a function of the observed data, for instance:
\begin{align*}
T(X_1,\ldots,X_{n_1}, Y_1,\ldots,Y_{n_2}) = \left| \frac{1}{n_1} \sum_{i=1}^{n_1} X_i - \frac{1}{n_2} \sum_{i=1}^{n_2} Y_i\right| := \tobs.
\end{align*} 
We let $N = n_1 + n_2$, and denote the permutations of the data by $\{Z_1,\ldots,Z_{N!}\}$. We let:
\begin{align*}
\permtest(\zobs) = \mathbb{I} \left[\left( \frac{1}{N!} \sum_{i=1}^{N!} \mathbb{I}( T(Z_i) > \tobs) \right) < \alpha\right].
\end{align*}
We claim that:
\begin{align*}
\mathbb{P}_{H_0}( \permtest(\zobs) = 1) \leq \alpha.
\end{align*}

{\bf Proof: } Note that the permutation test would reject the null only for test statistics that are in the upper $\alpha$-quantile of the distribution of test statistics, i.e.:
\begin{align*}
\alpha \geq  \frac{1}{N!} \sum_{i=1}^{N!} \permtest(Z_i).
\end{align*}
Taking the expectation over $Z_i$ under the null we obtain that,
\begin{align*}
\alpha \geq  \frac{1}{N!} \sum_{i=1}^{N!} \mathbb{E}_{H_0}[ \permtest(Z_i)].
\end{align*}
Under the null hypothesis each dataset $Z_i$ has the same distribution as $\zobs$ so we obtain that:
\begin{align*}
\alpha \geq  \frac{1}{N!} \sum_{i=1}^{N!} \mathbb{E}_{H_0}[ \permtest(\zobs)],
\end{align*}
i.e. that,
\begin{align*}
\mathbb{P}_{H_0}( \permtest(\zobs) = 1) \leq \alpha,
\end{align*}
as desired. Also note that upto some small quantization error (since the p-values that the permutation test produces are discrete) all of the above inequalities are actually equalities, i.e. the permutation test has Type I error that is very close to $\alpha$.



\section{Multiple Testing} 
The problem of multiple testing is one that is fundamental to a lot of science. 
Typical modern scientific discovery does not proceed in a simple fashion where we have a single hypothesis that we would like to test. 

A classical example is in the analysis of gene expression data. We measure the expression of tens of thousands of genes and we would like to know if any of them are associated with some phenotype (for example whether a person has a disease or not). Typically, the way this is done is that the scientist
does tens of thousands of hypothesis tests, and then reports the associations that are significant, i.e. reports the tests where the null hypothesis was rejected. 

This is very problematic: 

Suppose we did 1000 hypothesis tests, and for each of them rejected
the null when the p-value was less than $\alpha = 0.05$. How many times would you expect to falsely
reject the null hypothesis? 

The answer is we would expect to reject the null hypothesis 50 times. So we really cannot report all the discovered associations (rejections) as significant because we expect many false rejections.

The multiple testing problem is behind a lot of the ``reproducibility crisis'' of modern science. Many results that have been reported significant cannot be reproduced simply because they are false rejections. Too many false rejections come from doing multiple testing but not properly adjusting your tests to reflect the fact that many hypothesis tests are being done\footnote{Too many false rejections can also arise from tests that do not properly control the $\alpha$ level but this is usually easier to detect/fix.}.

The basic question is how to we adjust our p-value cutoffs to account for the fact that multiple tests are being done.

\subsection{The Family-Wise Error Rate}
We first need to define what the error control we desire is. Recall, the Type I error controls the probability of falsely rejecting the null hypothesis. We have seen that in order to control the Type I error
we can simply threshold the p-value, i.e rejecting the null if the p-value $\leq \alpha$ controls the Type I error at $\alpha$. 

%This is an important way to reason about multiple testing since it abstracts away the precise form of the test statistic and so on. Every hypothesis test we conduct gives us a p-value and we need to 

One possibility (and we will discuss a different one in the next lecture) is that when a scientist does multiple tests we care about controlling the probability that we falsely reject \emph{any} null hypothesis. This is called the Family-Wise Error Rate (FWER).

The FWER is the probability of falsely rejecting the null hypothesis even once amongst the multiple tests. 
%This is sometimes called testing the global null, i.e. testing the null hypothesis that all the null hypotheses

The basic question is then: how do we control the FWER?

\subsection{Sidak correction}
Suppose we do $d$ hypothesis tests, and want to control the FWER 
at $\alpha$. 

The Sidak correction says we reject any test if the p-value 
is smaller than:
\begin{align*}
\text{p-value} \leq 1 - (1 - \alpha)^{1/d} = \alpha_t,
\end{align*}
so we reject any test if its p-value is less than $\alpha_t$.

The main result is that: if the p-values are all \emph{independent} then the FWER $\leq \alpha$.

{\bf Proof: } Suppose that all the null hypotheses are true (this is called the \emph{global null}). You can easily see that if this is not the case you can simply ignore all the tests for which the null is false. 
The probability of falsely rejecting a fixed test is $\alpha_t$, so we correctly fail to reject it with probability $1 - \alpha_t$. 

Since the p-values are all independent the probability of falsely rejecting any null hypothesis is:
\begin{align*}
\text{FWER} = 1 - (1 - \alpha_t)^d = \alpha.
\end{align*}




\subsection{Bonferroni correction}
The main problem with the Sidak correction is that it requires the independence of p-values. 
This is unrealistic especially if you compute the test statistics for the different tests on the same
set of data. The Bonferroni
correction instead uses the union bound to avoid this assumption.

The Bonferroni correction says we reject any test if the p-value is smaller than:
\begin{align*}
\text{p-value} \leq \frac{\alpha}{d}.
\end{align*}

The main result is that: The FWER $\leq \alpha$.

{\bf Proof: } Suppose again that the global null is true. In this case,
\begin{align*}
\text{FWER} = \mathbb{P}\left(\bigcup_{i=1}^d \text{reject}~H_{0i}\right) \leq
\sum_{i=1}^d \mathbb{P} \left( \text{reject}~H_{0i}\right) \leq \sum_{i=1}^d \frac{\alpha}{d} = \alpha,
\end{align*}
where the first inequality follows from the union bound.


\section{Holm's procedure}
There are many possible improvements to the Bonferroni procedure. For instance, suppose that I told you that exactly (or at most) $d_0$ of the null hypotheses are truly nulls. Then you can see that we could have used the cut-off of $\frac{\alpha}{d_0}$ and still maintained control over the FWER.

As a thought experiment consider the following setting. You conduct $d = 5$ experiments and you observe p-values of $(0.7, 0.02,0, 0, 0)$.

Intuitively, it seems like since we are absolutely sure that the last three experiments are non-nulls we should be able to use the cut-off of $\alpha/2$ for the remaining two tests, and still control the FWER.

At a high-level it seems intuitively clear to us that other p-values for $\{p_j\}_{j \neq i}$ contain information at least about the number of null hypotheses and we can use this to relax the correction for $p_i$. Holm's procedure translates this intuition into a rigorous procedure.

\begin{enumerate}
\item Order the p-values $p_{(1)} \leq p_{(2)} \leq \ldots \leq p_{(d)}$.
\item If $p_{(1)} < \frac{\alpha}{d}$ then reject $H_{(1)}$ and move on, else stop and accept all $H_i$.
\item If $p_{(2)} < \frac{\alpha}{d - 1}$ then reject $H_{(2)}$ and move on, else stop and accept $H_{(2)}, \ldots, H_{(d)}$. \\
 \vdots
\item If $p_{(d)} < \alpha$, then reject $H_{(d)}$, else accept $H_{(d)}$.
\end{enumerate}
If you prefer: more succinctly, let 
\begin{align*}
i^* = \min \left\{ i: p_{(i)} > \frac{\alpha}{d - i + 1} \right\},
\end{align*}
and reject all $H_{(i)}$ for $i < i^*$.

The main claim is that Holm's procedure also controls the FWER at level $\alpha$. Importantly, Holm's procedure does not require independence of the p-values, and strictly dominates the Bonferroni procedure.

{\bf Proof: } Let $I_0$ denote the indices of the true nulls. First let us make an observation: if 
\begin{align*}
\min_{i \in I_0} p_i > \frac{\alpha}{d_0},
\end{align*}
then we reject none of the true nulls. This is because the first time we encounter a true null we would compare it to a threshold that is at most $\alpha/d_0$, and if we fail to reject it we would not reject any of the other true nulls.

So the FWER is:
\begin{align*}
\text{FWER} \leq \mathbb{P}\left(\min_{i \in I_0} p_i \leq \frac{\alpha}{d_0}\right) \leq \alpha,
\end{align*}
by the union bound.

\subsection{Something to think about} 
In the above discussion we assumed that there was a single scientist doing a bunch of tests so he could appropriately correct his procedure for the multiple testing problem. 

One thing to ponder is really what error rate should we be controlling, i.e. maybe I am the editor of a journal, and I want to ensure that across all  articles in my journal the FWER is $\leq \alpha$. Maybe I want this to be true across the entire field? Should I be adjusting my p-values for people in other disciplines? Sounds absurd but it actually makes sense if you think about each of these procedures and their implications for reproducibility.


\section{False Discovery Rate}
Today we will focus on motivating this idea and then discuss a way to control the FDR in the next lecture. 

Suppose that we tested $d = 1000$ genes for association with some disease, we got a 1000 p-values, and 100 of them were less than $0.01$.
We'd expect that roughly $0.01 d_0 \leq 0.01 d = 10$ of these to be falsely rejected nulls, and perhaps this is not a bad tradeoff, i.e. if we rejected the first 100 nulls, we would spend only 10\% of our time on falsely rejected nulls, i.e. we would make 90 real discoveries. 

The FDR is the expected number
of false rejections divided by the number of rejections. 

Denote the number of false rejections as $V$, and the total number of rejections as $R$.
Then the false discovery \emph{proportion} is:
\begin{align*}
\text{FDP} = \begin{cases}
\frac{V}{R} ~~\text{if}~~R > 0 \\
0~~~\text{if}~~R = 0.
\end{cases}
\end{align*}
The FDR is then defined as:
\begin{align*}
\text{FDR} = \mathbb{E}[\text{FDP}].
\end{align*}
In this notation we can see that the FWER is:
\begin{align*}
\text{FWER} = \mathbb{P}(V \geq 1).
\end{align*}

We will next consider how one can control the FDR. We will describe a procedure known as the
Benjamini-Hochberg (BH) procedure.

\subsection{The BH procedure}
The BH procedure is one that controls the FDR under independence (i.e. the p-values are independent). There is a much weaker form of this procedure that works under dependence (see the Wasserman book).
It turns out to be very challenging to tightly control FDR under strong dependence. 

The procedure is:
\begin{enumerate}
\item Suppose we do $d$ tests. 
Let us take the p-values $p_1,\ldots,p_d$, and sort them, i.e. we create the list:
$p_{(1)} \leq p_{(2)} \leq \ldots \leq p_{(d)}$.
\item Define the thresholds: 
\begin{align*}
t_i = \frac{i \alpha}{d}.
\end{align*}
\item Find the largest $i_{\max}$ such that
\begin{align*}
i_{\max} = \arg \max_i \{i: p_{(i)} < t_i \}.
\end{align*}
\item Reject all nulls upto and including $i_{\max}$.
\end{enumerate}
This might seem a bit confusing but here is a simple picture:
\begin{center}
\includegraphics[scale=0.7]{fdr}
\end{center}
\subsection{Properties of FDR}
We have now seen a procedure that controls the FDR under some assumptions. One question of
interest is how does FDR control compare to FWER control? Another is just: how do we interpret FDR control?

{\bf Interpreting FDR control: } 
The way to think about FDR control is: if we repeat our experiment many times, on average we
control the FDP. This is not a statement about the individual experiment we did conduct, and
really it does not say much about how likely it is that on a given experiment we 
have an FDP that is larger than a threshold (think about using Markov's inequality). 

FWER on the other hand, does control the error rate for a single experiment. 
That is, with FWER control, we have managed our false discoveries unless we are
very unlucky; with FDR control, on average our test will control FDP, but in our particular experiment  we may not
have done a very good job. We will see in a second controlling FWER does control the FDR. The way to interpret all of this is that: FDR control is a very weak notion of error control.

{\bf Connection to FWER: } 
\begin{enumerate}
\item The first connection is that under the global null (when all the null hypotheses are true) FDR control is equivalent to FWER control.

{\bf Proof: } Under the global null, any rejection is a false rejection. 
There are two possibilities: either we do not reject anything: in this case the FDP = 0. If we do reject any null hypothesis then our FDP is 1 (since $V = R$). So we have that:
\begin{align*}
\text{FDR} = \mathbb{E}[\text{FDP}] = \mathbb{P}(V > 0)*1 + \mathbb{P}(V = 0)*0 = \mathbb{P}(V > 0)
= \text{FWER}.
\end{align*}

\item The second connection is that the FWER $\geq \text{FDR}$ always. This implies that controlling the FWER implies FDR control. 

{\bf Proof: } We can see that the following is a simple upper bound on the FDP:
\begin{align*}
\text{FDP} \leq \mathbb{I}(V \geq 1),
\end{align*}
since if $V = 0$, FDP = 0, and if $V > 0$ then $V/R \leq 1$. Taking expectations of this expression
gives:
\begin{align*}
\text{FDR} \leq \mathbb{P}(V \geq 1) = \text{FWER}.
\end{align*}
The flip-side of this is that FDR control is less stringent so if this is the correct measure for you then you will have \emph{more} power by controlling FDR (rather than controlling FWER).
\end{enumerate}



\end{document}





